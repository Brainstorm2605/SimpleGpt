{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T09:59:45.733090Z","iopub.status.busy":"2024-03-22T09:59:45.732695Z","iopub.status.idle":"2024-03-22T10:00:00.290475Z","shell.execute_reply":"2024-03-22T10:00:00.289392Z","shell.execute_reply.started":"2024-03-22T09:59:45.733061Z"},"trusted":true},"outputs":[],"source":["pip install tiktoken"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T10:00:00.292863Z","iopub.status.busy":"2024-03-22T10:00:00.292520Z","iopub.status.idle":"2024-03-22T10:00:00.324683Z","shell.execute_reply":"2024-03-22T10:00:00.324003Z","shell.execute_reply.started":"2024-03-22T10:00:00.292837Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","import pickle\n","import mmap\n","import random\n","import os\n","import logging \n","import tiktoken\n","import re\n","import unicodedata\n","import time"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T10:01:09.933968Z","iopub.status.busy":"2024-03-22T10:01:09.933283Z","iopub.status.idle":"2024-03-22T10:01:09.942647Z","shell.execute_reply":"2024-03-22T10:01:09.941396Z","shell.execute_reply.started":"2024-03-22T10:01:09.933934Z"},"trusted":true},"outputs":[],"source":["batch_size = 32 # how many independent sequences will we process in parallel?\n","context_win = 128 # what is the maximum context length for predictions?\n","learning_rate = 3e-5\n","device = 'cuda' if torch.cuda.is_available() else 'cpu' #use this when running on nvidia gpu\n","n_embd = 1024\n","n_head = 8\n","n_layer = 12\n","max_epochs = 5 \n","dropout = 0.2\n","vocab_size = 100277 \n","torch.manual_seed(1337)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T10:01:09.944764Z","iopub.status.busy":"2024-03-22T10:01:09.944413Z","iopub.status.idle":"2024-03-22T10:01:09.951349Z","shell.execute_reply":"2024-03-22T10:01:09.950551Z","shell.execute_reply.started":"2024-03-22T10:01:09.944736Z"},"trusted":true},"outputs":[],"source":["# #only for TPU run\n","# num_cores = xm.xrt_world_size()\n","# # Select the TPU device\n","# device = xm.xla_device()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T10:01:09.952676Z","iopub.status.busy":"2024-03-22T10:01:09.952406Z","iopub.status.idle":"2024-03-22T10:01:09.963422Z","shell.execute_reply":"2024-03-22T10:01:09.962588Z","shell.execute_reply.started":"2024-03-22T10:01:09.952653Z"},"trusted":true},"outputs":[],"source":["enc = tiktoken.get_encoding(\"cl100k_base\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T10:01:09.964748Z","iopub.status.busy":"2024-03-22T10:01:09.964459Z","iopub.status.idle":"2024-03-22T10:01:09.972314Z","shell.execute_reply":"2024-03-22T10:01:09.971481Z","shell.execute_reply.started":"2024-03-22T10:01:09.964724Z"},"trusted":true},"outputs":[],"source":["eval_interval = 1000\n","eval_iters = 200"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T10:01:09.975054Z","iopub.status.busy":"2024-03-22T10:01:09.974673Z","iopub.status.idle":"2024-03-22T10:01:09.982917Z","shell.execute_reply":"2024-03-22T10:01:09.981939Z","shell.execute_reply.started":"2024-03-22T10:01:09.975007Z"},"trusted":true},"outputs":[],"source":["device"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T10:01:09.984341Z","iopub.status.busy":"2024-03-22T10:01:09.984034Z","iopub.status.idle":"2024-03-22T10:01:09.990834Z","shell.execute_reply":"2024-03-22T10:01:09.989845Z","shell.execute_reply.started":"2024-03-22T10:01:09.984317Z"},"trusted":true},"outputs":[],"source":["train_file = r\"\" #location of training file\n","val_file = r\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T10:01:10.000199Z","iopub.status.busy":"2024-03-22T10:01:09.999906Z","iopub.status.idle":"2024-03-22T10:01:10.011194Z","shell.execute_reply":"2024-03-22T10:01:10.010244Z","shell.execute_reply.started":"2024-03-22T10:01:10.000177Z"},"trusted":true},"outputs":[],"source":["def get_batch(split):\n","    global current_position \n","    filename = train_file if split == 'train' else val_file\n","    filesize = os.path.getsize(filename)  # Get size of the file using 'os'\n","    encodings = ['utf-8']\n","    for encoding in encodings:\n","        try:\n","            with open(filename, 'r', encoding=encoding,errors='replace') as f:\n","                f.seek(current_position)\n","\n","                data_chunk = ''\n","                while len(data_chunk) < context_win * batch_size:\n","                    additional_data = f.read(context_win * batch_size - len(data_chunk))\n","                    data_chunk += additional_data\n","\n","                if not data_chunk:  \n","                    current_position = 0  \n","                    return None \n","                data = torch.tensor(enc.encode(data_chunk), dtype=torch.long)  \n","                \n","                current_position += len(data_chunk)  \n","\n","                ix = torch.randint(len(data) - context_win, (batch_size,))\n","                x = torch.stack([data[i:i+context_win] for i in ix])\n","                y = torch.stack([data[i+1:i+context_win+1] for i in ix])\n","                x, y = x.to(device), y.to(device)\n","\n","                return x, y \n","        except UnicodeDecodeError:\n","            print(\"UnicodeDecodeError\")   \n","    return None"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T10:01:10.012598Z","iopub.status.busy":"2024-03-22T10:01:10.012333Z","iopub.status.idle":"2024-03-22T10:01:10.021233Z","shell.execute_reply":"2024-03-22T10:01:10.020363Z","shell.execute_reply.started":"2024-03-22T10:01:10.012576Z"},"trusted":true},"outputs":[],"source":["@torch.no_grad()\n","def estimate_loss():\n","        out = {}\n","        model.eval()\n","        for split in ['train', 'val']:\n","            losses = torch.zeros(eval_iters)\n","            for k in range(eval_iters):\n","                X, Y = get_batch(split)\n","                if X is None:  # Handle case when get_batch returns None\n","                    continue\n","                logits, loss = model(X, Y)\n","                losses[k] = loss.item()\n","            out[split] = losses.mean()\n","        model.train()\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T10:01:10.121333Z","iopub.status.busy":"2024-03-22T10:01:10.120849Z","iopub.status.idle":"2024-03-22T10:01:10.132220Z","shell.execute_reply":"2024-03-22T10:01:10.131040Z","shell.execute_reply.started":"2024-03-22T10:01:10.121292Z"},"trusted":true},"outputs":[],"source":["class Head(nn.Module):\n","    def __init__(self,head_size):\n","        super().__init__()\n","        self.key = nn.Linear(n_embd,head_size)\n","        self.query = nn.Linear(n_embd,head_size)\n","        self.value = nn.Linear(n_embd,head_size)\n","        self.register_buffer('tril',torch.tril(torch.ones(context_win,context_win)))\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self,x):\n","        B,T,C = x.shape\n","        k = self.key(x)\n","        q = self.query(x)\n","        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5\n","        wei = wei.masked_fill(self.tril[:T,:T]==0,float('-inf'))\n","        wei = F.softmax(wei,dim=-1)\n","        wei = self.dropout(wei)\n","        v = self.value(x)\n","        out = wei @ v\n","        return out\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T10:01:10.134462Z","iopub.status.busy":"2024-03-22T10:01:10.133900Z","iopub.status.idle":"2024-03-22T10:01:10.142522Z","shell.execute_reply":"2024-03-22T10:01:10.141632Z","shell.execute_reply.started":"2024-03-22T10:01:10.134435Z"},"trusted":true},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(head_size * num_heads, n_embd)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)\n","        out = self.dropout(self.proj(out))\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T10:01:10.144299Z","iopub.status.busy":"2024-03-22T10:01:10.143709Z","iopub.status.idle":"2024-03-22T10:01:10.151453Z","shell.execute_reply":"2024-03-22T10:01:10.150521Z","shell.execute_reply.started":"2024-03-22T10:01:10.144264Z"},"trusted":true},"outputs":[],"source":["class FeedFoward(nn.Module):\n","    def __init__(self, n_embd):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embd, 4 * n_embd),\n","            nn.ReLU(),\n","            nn.Linear(4 * n_embd, n_embd),\n","            nn.Dropout(dropout),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T10:01:10.154358Z","iopub.status.busy":"2024-03-22T10:01:10.154012Z","iopub.status.idle":"2024-03-22T10:01:10.161489Z","shell.execute_reply":"2024-03-22T10:01:10.160680Z","shell.execute_reply.started":"2024-03-22T10:01:10.154327Z"},"trusted":true},"outputs":[],"source":["class Block(nn.Module):\n","    def __init__(self, n_embd, n_head):\n","        super().__init__()\n","        head_size = n_embd // n_head\n","        self.sa = MultiHeadAttention(n_head, head_size)\n","        self.ffwd = FeedFoward(n_embd)\n","        self.ln1 = nn.LayerNorm(n_embd)\n","        self.ln2 = nn.LayerNorm(n_embd)\n","\n","    def forward(self, x):\n","        x = x + self.sa(self.ln1(x))\n","        x = x + self.ffwd(self.ln2(x))\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T10:01:10.162957Z","iopub.status.busy":"2024-03-22T10:01:10.162672Z","iopub.status.idle":"2024-03-22T10:01:23.360320Z","shell.execute_reply":"2024-03-22T10:01:23.357722Z","shell.execute_reply.started":"2024-03-22T10:01:10.162934Z"},"trusted":true},"outputs":[],"source":["class GPTModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.token_embedding_table = nn.Embedding(vocab_size,n_embd)\n","        self.position_embedding = nn.Embedding(context_win,n_embd)\n","        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n","        self.ln_f = nn.LayerNorm(n_embd) \n","        self.lm_head = nn.Linear(n_embd, vocab_size)\n","        \n","        self.apply(self._init_weights)\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, nn.Embedding):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)    \n","        \n","    def forward(self,index,target=None):\n","        B,T = index.shape\n","        \n","        token = self.token_embedding_table(index)\n","        pos_embd = self.position_embedding(torch.arange(T,device=device))\n","        x = token + pos_embd\n","        x = self.blocks(x)\n","        x = self.ln_f(x)\n","        logits = self.lm_head(x)\n","        \n","        if target == None:\n","            loss = None\n","        else:    \n","            B,T,C = logits.shape\n","            logits = logits.view(B*T,C)\n","            target = target.view(B*T)\n","            loss = F.cross_entropy(logits,target)\n","            \n","        return logits,loss    \n","\n","            \n","    def generate(self,index,max_size):\n","        for _ in range(max_size):\n","            index_cropped = index[:,-context_win:]\n","            logits,loss = self(index_cropped)\n","            logits = logits[:,-1,:]\n","            prob = F.softmax(logits,dim=-1)\n","            next_word = torch.multinomial(prob,num_samples=1)\n","            index = torch.cat((index,next_word),dim = 1)\n","        return index    \n","\n","model = GPTModel() \n","model = model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-22T10:01:23.361294Z","iopub.status.idle":"2024-03-22T10:01:23.361671Z","shell.execute_reply":"2024-03-22T10:01:23.361499Z","shell.execute_reply.started":"2024-03-22T10:01:23.361483Z"},"trusted":true},"outputs":[],"source":["#checkpoint = torch.load(r'', map_location=torch.device('cuda')) # load model if already has a model\n","#model.load_state_dict(checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-22T10:01:23.363483Z","iopub.status.idle":"2024-03-22T10:01:23.364023Z","shell.execute_reply":"2024-03-22T10:01:23.363786Z","shell.execute_reply.started":"2024-03-22T10:01:23.363751Z"},"trusted":true},"outputs":[],"source":["print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-22T10:01:23.367137Z","iopub.status.idle":"2024-03-22T10:01:23.367626Z","shell.execute_reply":"2024-03-22T10:01:23.367391Z","shell.execute_reply.started":"2024-03-22T10:01:23.367371Z"},"trusted":true},"outputs":[],"source":["optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-22T10:01:23.369540Z","iopub.status.idle":"2024-03-22T10:01:23.370313Z","shell.execute_reply":"2024-03-22T10:01:23.370088Z","shell.execute_reply.started":"2024-03-22T10:01:23.370057Z"},"trusted":true},"outputs":[],"source":["num_batches_per_epoch = 2200000\n","num_batches_per_epoch"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-22T10:01:23.371757Z","iopub.status.idle":"2024-03-22T10:01:23.372204Z","shell.execute_reply":"2024-03-22T10:01:23.371971Z","shell.execute_reply.started":"2024-03-22T10:01:23.371953Z"},"trusted":true},"outputs":[],"source":["save_dir = r''\n","os.makedirs(save_dir, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-22T10:01:23.373947Z","iopub.status.idle":"2024-03-22T10:01:23.374345Z","shell.execute_reply":"2024-03-22T10:01:23.374183Z","shell.execute_reply.started":"2024-03-22T10:01:23.374167Z"},"trusted":true},"outputs":[],"source":["def save_model(model, counter):\n","    model_path = os.path.join(save_dir, f'model_iter_{counter}.pt')\n","    torch.save(model.module.state_dict(), model_path)\n","    print(f\"Model saved at iteration {counter}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-22T10:01:23.375633Z","iopub.status.idle":"2024-03-22T10:01:23.376049Z","shell.execute_reply":"2024-03-22T10:01:23.375841Z","shell.execute_reply.started":"2024-03-22T10:01:23.375825Z"},"trusted":true},"outputs":[],"source":["def calculate_perplexity(model):\n","    total_loss = 0\n","    total_tokens = 0\n","    num_batches_per_validation = 32  # Define the number of batches for validation\n","    \n","    # Set the model to evaluation mode\n","    model.eval()\n","    \n","    with torch.no_grad():\n","        for _ in range(num_batches_per_validation):\n","            try:\n","                # Get a batch of validation data using get_batch() function\n","                xb, yb = get_batch('val')  # Assuming 'val' is the split for validation data\n","                \n","                if xb is None:\n","                    break  # End of validation data\n","                \n","                # Forward pass and loss estimation\n","                losses = estimate_loss()  # Using estimate_loss() function\n","                loss = losses['val']\n","                \n","                # Update total loss and total tokens\n","                total_loss += loss.item() * yb.numel()  # Multiplying loss by number of tokens\n","                total_tokens += yb.numel()  # Count the number of tokens in targets (yb)\n","            \n","            except Exception as e:\n","                continue\n","    \n","    # Calculate perplexity\n","    average_loss = total_loss / total_tokens\n","    perplexity = torch.exp(torch.tensor(average_loss))\n","    \n","    return perplexity\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-22T10:01:23.377710Z","iopub.status.idle":"2024-03-22T10:01:23.378106Z","shell.execute_reply":"2024-03-22T10:01:23.377897Z","shell.execute_reply.started":"2024-03-22T10:01:23.377883Z"},"trusted":true},"outputs":[],"source":["current_position = 0\n","counter = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-22T10:01:23.379720Z","iopub.status.idle":"2024-03-22T10:01:23.380112Z","shell.execute_reply":"2024-03-22T10:01:23.379907Z","shell.execute_reply.started":"2024-03-22T10:01:23.379893Z"},"trusted":true},"outputs":[],"source":["for epoch in range(max_epochs):\n","    for iter in range(num_batches_per_epoch):\n","        # Sample a batch of data (random or sequential, based on your get_batch implementation)\n","            start = time.time()\n","            xb, yb = get_batch('train')\n","            if xb is None:\n","                print('none')\n","                continue\n","            else:\n","                # Update the counter for every iteration\n","                counter += 1\n","\n","                # Evaluate the loss\n","                logits, loss = model(xb, yb)\n","                optimizer.zero_grad(set_to_none=True)\n","                loss.backward()\n","                optimizer.step()\n","                if counter % eval_interval == 0 or epoch == max_epochs - 1:\n","                                endtime = time.time() - start\n","                                losses = estimate_loss() \n","                                print(f\"Iteration {counter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, total time taken: {endtime}\")\n","                                print(enc.decode(model.generate(torch.zeros((1,1),dtype=torch.long,device=device),max_size=500)[0].tolist()))\n","                                print('\\n')\n","                                print(\"------------\")\n","                # Check if it's time to save the model\n","                if counter % 100 == 0 :\n","                    print(\".\",end=\"\", flush=True)\n","                if counter % 5000 == 0:\n","                    save_model(model, counter)\n","                    \n","                # Reset current_position if we've reached the end of the epoch \n","                if iter == num_batches_per_epoch - 1: \n","                    current_position = 0\n"," "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-22T10:01:23.381736Z","iopub.status.idle":"2024-03-22T10:01:23.382259Z","shell.execute_reply":"2024-03-22T10:01:23.382023Z","shell.execute_reply.started":"2024-03-22T10:01:23.381975Z"},"trusted":true},"outputs":[],"source":["current_position,counter"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-22T10:01:23.383485Z","iopub.status.idle":"2024-03-22T10:01:23.383838Z","shell.execute_reply":"2024-03-22T10:01:23.383679Z","shell.execute_reply.started":"2024-03-22T10:01:23.383665Z"},"trusted":true},"outputs":[],"source":["perplexity = calculate_perplexity(model)\n","print(f\"Iteration {counter}: Perplexity: {perplexity}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-22T10:01:23.385170Z","iopub.status.idle":"2024-03-22T10:01:23.385555Z","shell.execute_reply":"2024-03-22T10:01:23.385378Z","shell.execute_reply.started":"2024-03-22T10:01:23.385362Z"},"trusted":true},"outputs":[],"source":["print(enc.decode(model.generate(torch.zeros((1,1),dtype=torch.long,device=device),max_size=1500)[0].tolist()))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-22T10:01:23.386564Z","iopub.status.idle":"2024-03-22T10:01:23.386933Z","shell.execute_reply":"2024-03-22T10:01:23.386766Z","shell.execute_reply.started":"2024-03-22T10:01:23.386750Z"},"trusted":true},"outputs":[],"source":["print(f\"Iteration {counter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-22T10:01:23.389141Z","iopub.status.idle":"2024-03-22T10:01:23.389633Z","shell.execute_reply":"2024-03-22T10:01:23.389399Z","shell.execute_reply.started":"2024-03-22T10:01:23.389378Z"},"trusted":true},"outputs":[],"source":["save_model(model, counter)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4517792,"sourceId":7731388,"sourceType":"datasetVersion"},{"modelInstanceId":10914,"sourceId":13197,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
